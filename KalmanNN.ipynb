{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training with the Extended Kalman Filter\n",
    "This notebook demonstrates how to train a neural network with an EKF. I use a teacher network to produce data, and train a student network to model the input-output relation defined by the teacher network through examples. I use the same architecture for the two networks, but this is not a restriction in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main class for the EKF based trainer. The trainer takes as input an NN object, along with some parameters. It is assumed that the student NN parameters perform a random walk. The training data is constructed by feeding random inputs to the teacher NN. At each step, the trainer is provided either a single input-output pair, or a batch of input-output pairs. The weights of the NN are updated so that the random walk of the student weights approach a set of values such that the student produces similar outputs as the teacher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    # Kalman filter trainer for a differentiable neural network\n",
    "    # full covariance matrix is kept throughout the iterations\n",
    "    def __init__(self, student, params):\n",
    "        self.C = params['Cov'] # initial covariance matrix\n",
    "        self.num_param = torch.numel(torch.nn.utils.parameters_to_vector(student.parameters())) # number of elements\n",
    "        self.RW = params['state_noise'] # noise for the definition of random walk on the state\n",
    "        self.obs_noise = params['obs_noise'] # observation noise variance\n",
    "        self.student = student # the student to be trained\n",
    "        \n",
    "    def vectorize_grad(self):\n",
    "        # returns the gradients of all of the parameters in a vector\n",
    "        # somehow I was not able to obtain this information using \n",
    "        # torch.nn.utils.parameters_to_vector\n",
    "        # that function gives the values of the parameters, but not the gradients\n",
    "\n",
    "        vec = []\n",
    "        for param in self.student.parameters():\n",
    "            vec.append(param.grad.view(-1))\n",
    "\n",
    "        return torch.cat(vec)\n",
    "\n",
    "    def Jacobian(self, x):\n",
    "        # evaluates z = model(x,w) and the Jacobian of g(w) = model(x,w) at w\n",
    "        # returns z along with the mentioned Jacobian\n",
    "        self.student.zero_grad()\n",
    "        z = self.student(x)\n",
    "        H = torch.empty(torch.numel(z), self.num_param)\n",
    "        for k, zk in enumerate(z.view(-1)):\n",
    "            self.student.zero_grad()\n",
    "            zk.backward(retain_graph = True)\n",
    "            H[k,:] = self.vectorize_grad()\n",
    "        return z, H\n",
    "    \n",
    "    def Update(self, x, y):\n",
    "        ## prediction step\n",
    "        # it is assumed that the weights are doing a random walk, so just update the covariance\n",
    "        self.C += self.RW * torch.eye(self.num_param)\n",
    "        \n",
    "        ## update step\n",
    "        z, H = self.Jacobian(x)\n",
    "        \n",
    "        error = y - z\n",
    "        HP = torch.mm(H, self.C)\n",
    "        HPH = torch.mm(HP, H.t()) + self.obs_noise * torch.eye(torch.numel(y))\n",
    "        update = torch.solve(error.view(-1,1), HPH)[0].view(-1)\n",
    "        mean = torch.nn.utils.parameters_to_vector(self.student.parameters()) + torch.mm(HP.t(),update.view(-1,1)).view(-1)\n",
    "        update2 = torch.solve(HP, HPH)[0]\n",
    "        self.C -= torch.mm(HP.t(), update2)\n",
    "        self.C = (self.C + self.C.t())/2\n",
    "        torch.nn.utils.vector_to_parameters(mean, self.student.parameters())# this converts the parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer is defined, we now declare the class for the student and teacher networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [5,10,10,5] # architecture \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(K[0],K[1])        \n",
    "        self.layer2 = nn.Linear(K[1],K[2])\n",
    "        self.layer3 = nn.Linear(K[2],K[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.softsign(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.softsign(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the teacher and student networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = Net()\n",
    "student = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the trainer. Note that we provide the student network to the trainer. This is similar to providing the student NN to a Pytorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = torch.nn.utils.parameters_to_vector(student.parameters())\n",
    "params = {}\n",
    "\n",
    "# initial value of the covariance of the weights\n",
    "params['Cov'] = 1e-5 * torch.eye(torch.numel(total)) \n",
    "\n",
    "# the variance defining the random walk of the weights\n",
    "params['state_noise'] = 1e-8 \n",
    "\n",
    "# noise variance for the observations. \n",
    "# The observation noise is actually zero since the student has the capability to perfectly mimic the teacher\n",
    "params['obs_noise'] = 1e-5 \n",
    "\n",
    "train_st = Trainer(student, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the student network for MAX_ITER steps. Here, we input single instances of input-output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log mse: -1.1157795190811157\n",
      "log mse: -2.36008358001709\n",
      "log mse: -2.557671308517456\n",
      "log mse: -2.6477041244506836\n",
      "log mse: -2.6884725093841553\n",
      "log mse: -2.737579822540283\n",
      "log mse: -2.7889180183410645\n",
      "log mse: -2.8262202739715576\n",
      "log mse: -2.8347623348236084\n",
      "log mse: -2.877030611038208\n"
     ]
    }
   ],
   "source": [
    "MAX_ITER = 1000\n",
    "test_size = 100\n",
    "test_in = torch.FloatTensor(np.random.normal(0,1,(K[0],test_size)))\n",
    "for iter in range(MAX_ITER):\n",
    "    # generate a random input, and compute the output using the teacher\n",
    "    x = torch.FloatTensor(np.random.normal(0,1,K[0]))\n",
    "    x.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        y = teacher(x)\n",
    "\n",
    "    train_st.Update(x, y)\n",
    "    \n",
    "    # check mse\n",
    "    if np.mod(iter, 100) == 0:\n",
    "        with torch.no_grad():\n",
    "            y = teacher(test_in.t())\n",
    "            z = student(test_in.t())\n",
    "        mse = sum((y - z).view(-1)**2 / test_size)\n",
    "        print(\"log mse: {}\".format(torch.log10(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the student network for MAX_ITER steps. Here, we input multiple input-output pairs -- this is similar to using batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log mse: -2.8665785789489746\n",
      "log mse: -3.0722601413726807\n",
      "log mse: -3.1949334144592285\n",
      "log mse: -3.2839367389678955\n",
      "log mse: -3.373305559158325\n",
      "log mse: -3.475947380065918\n",
      "log mse: -3.530769109725952\n",
      "log mse: -3.57464599609375\n",
      "log mse: -3.6186318397521973\n",
      "log mse: -3.6387879848480225\n"
     ]
    }
   ],
   "source": [
    "MAX_ITER = 1000\n",
    "batch = 10\n",
    "test_size = 100\n",
    "test_in = torch.FloatTensor(np.random.normal(0,1,(K[0],test_size)))\n",
    "for iter in range(MAX_ITER):\n",
    "    # generate a random input\n",
    "    x = torch.FloatTensor(np.random.normal(0,1,(K[0], batch)))\n",
    "    x.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        y = teacher(x.t())\n",
    "\n",
    "    train_st.Update(x.t(), y)\n",
    "    # check mse\n",
    "    if np.mod(iter, 100) == 0:\n",
    "        with torch.no_grad():\n",
    "            y = teacher(test_in.t())\n",
    "            z = student(test_in.t())\n",
    "        mse = sum((y - z).view(-1)**2 / test_size)\n",
    "        print(\"log mse: {}\".format(torch.log10(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ilker Bayram, ibayram@ieee.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
